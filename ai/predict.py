import joblib
import os
import sys
from datetime import datetime, timedelta
from contextlib import redirect_stdout, redirect_stderr
from io import StringIO

# Suppress vnstock output by redirecting to stderr
import io
with redirect_stdout(sys.stderr):
    import vnstock

from feature_engineering import prepare_features

# Custom logger to stderr
def log(*args, **kwargs):
    print(*args, file=sys.stderr, **kwargs)

def predict_stock(symbol='VCB'):
    """
    D·ª± ƒëo√°n gi√° c·ªï phi·∫øu cho ng√†y mai
    """
    log("\n" + "="*60)
    log(f"üîÆ D·ª∞ ƒêO√ÅN GI√Å C·ªî PHI ·∫æU {symbol}")
    log("="*60)
    
    # 1. Load model
    try:
        model = joblib.load(f'models/model_{symbol}_advanced.pkl')
        features_list = joblib.load(f'models/features_{symbol}.pkl')
        log(f"‚úì Loaded advanced model with {len(features_list)} features")
    except Exception as e:
        log(f"‚ùå Error: {e}")
        log(f"   Please train the model first: python ai/model_training_advanced.py")
        return
    
    # 2. Fetch latest data
    log(f"\nüì• Fetching latest data...")
    end_date = datetime.now().strftime('%Y-%m-%d')
    start_date = (datetime.now() - timedelta(days=90)).strftime('%Y-%m-%d')
    
    quote = vnstock.Quote(symbol=symbol, source='VCI')
    df_raw = quote.history(start=start_date, end=end_date, interval='1D')
    
    if df_raw.empty:
        log("‚ùå No data available")
        return
    
    # 3. Prepare features
    log(f"üîß Calculating technical indicators...")
    df_processed = prepare_features(df_raw, symbol, start_date, end_date, vnstock)
    
    # 4. Get latest features
    X = df_processed[features_list]
    latest_features = X.iloc[-1:].values
    
    # 5. Make prediction
    prediction = model.predict(latest_features)[0]
    latest_close = df_processed.iloc[-1]['close']
    change = prediction - latest_close
    change_pct = (change / latest_close) * 100
    
    # 6. Display results
    latest_date = df_processed.iloc[-1]['time']
    # 6. Display results
    latest_date = df_processed.iloc[-1]['time']
    log(f"\nüìä K·∫æT QU·∫¢ D·ª∞ ƒêO√ÅN:")
    log(f"   {'-'*50}")
    log(f"   Ng√†y g·∫ßn nh·∫•t:      {latest_date}")
    log(f"   Gi√° ƒë√≥ng c·ª≠a:       {latest_close:,.0f} VND")
    log(f"   D·ª± ƒëo√°n ng√†y mai:   {prediction:,.0f} VND")
    log(f"   Thay ƒë·ªïi d·ª± ki·∫øn:   {change:+,.0f} VND ({change_pct:+.2f}%)")
    
    if change_pct > 2:
        log(f"   Xu h∆∞·ªõng:           üìà TƒÇNG M·∫†NH")
    elif change_pct > 0:
        log(f"   Xu h∆∞·ªõng:           üìà TƒÉng nh·∫π")
    elif change_pct > -2:
        log(f"   Xu h∆∞·ªõng:           üìâ Gi·∫£m nh·∫π")
    else:
        log(f"   Xu h∆∞·ªõng:           üìâ GI·∫¢M M·∫†NH")
    
    # 7. Show some key indicators
    log(f"\nüìà CH·ªà S·ªê K·ª∏ THU·∫¨T HI·ªÜN T·∫†I:")
    log(f"   {'-'*50}")
    latest_row = df_processed.iloc[-1]
    
    if 'RSI' in df_processed.columns:
        rsi = latest_row['RSI']
        rsi_signal = "Qu√° mua" if rsi > 70 else "Qu√° b√°n" if rsi < 30 else "Trung l·∫≠p"
        log(f"   RSI:                {rsi:.2f} ({rsi_signal})")
    
    if 'MACD' in df_processed.columns:
        macd = latest_row['MACD']
        macd_signal = latest_row.get('MACD_signal', 0)
        trend = "T√≠ch c·ª±c" if macd > macd_signal else "Ti√™u c·ª±c"
        log(f"   MACD:               {macd:.2f} ({trend})")
    
    if 'BB_position' in df_processed.columns:
        bb_pos = latest_row['BB_position']
        bb_status = "Tr√™n d·∫£i" if bb_pos > 0.8 else "D∆∞·ªõi d·∫£i" if bb_pos < 0.2 else "Gi·ªØa d·∫£i"
        log(f"   Bollinger Bands:    {bb_pos:.2f} ({bb_status})")
    
    if 'EMA_12' in df_processed.columns and 'EMA_26' in df_processed.columns:
        ema_diff = latest_row['EMA_12'] - latest_row['EMA_26']
        ema_trend = "Xu h∆∞·ªõng tƒÉng" if ema_diff > 0 else "Xu h∆∞·ªõng gi·∫£m"
        log(f"   EMA Cross:          {ema_diff:+.2f} ({ema_trend})")
    
    # 8. Feature importance
    if hasattr(model, 'feature_importances_'):
        log(f"\nüîç Y·∫æU T·ªê QUAN TR·ªåNG NH·∫§T:")
        log(f"   {'-'*50}")
        importance = sorted(
            zip(features_list, model.feature_importances_),
            key=lambda x: x[1],
            reverse=True
        )
        for i, (feature, imp) in enumerate(importance[:5], 1):
            log(f"   {i}. {feature:<20} ({imp*100:.1f}%)")
    
    log(f"\n{'='*60}\n")
    return

def get_prediction_data(symbol='VCB'):
    """
    L·∫•y d·ªØ li·ªáu d·ª± ƒëo√°n d∆∞·ªõi d·∫°ng dictionary cho Backend
    With intelligent caching to improve performance
    """
    try:
        # Check cache first
        current_dir = os.path.dirname(os.path.abspath(__file__))
        cache_dir = os.path.join(current_dir, 'cache')
        os.makedirs(cache_dir, exist_ok=True)
        cache_file = os.path.join(cache_dir, f'prediction_{symbol}.json')
        
        # Cache TTL: 30 minutes
        cache_ttl = 30 * 60  # seconds
        
        if os.path.exists(cache_file):
            cache_age = datetime.now().timestamp() - os.path.getmtime(cache_file)
            if cache_age < cache_ttl:
                # Cache is fresh, return it
                log(f"‚úì Using cached prediction for {symbol} (age: {int(cache_age/60)}min)")
                import json
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cached_result = json.load(f)
                    cached_result['cached'] = True
                    cached_result['cache_age_minutes'] = int(cache_age / 60)
                    return cached_result
        
        # Load model and features
        model_path = os.path.join(current_dir, 'models', f'model_{symbol}_advanced.pkl')
        features_path = os.path.join(current_dir, 'models', f'features_{symbol}.pkl')
        
        if not os.path.exists(model_path):
             # Try to train on demand
             try:
                 # Import only here to avoid circular imports at top level if any
                 from model_training_advanced import train_and_save_model
                 
                 # Redirect stdout/stderr to suppress training logs from polluting JSON output request
                 # But we might want to log somewhere. For now, just suppress or log to stderr
                 # Since `worker.py` captures stdout for JSON, anything printed to stdout during training will break it
                 # So we MUST suppress stdout or redirect it to stderr
                 
                 # Note: train_and_save_model prints to stdout. We need to capture that.
                 # Actually, let's just log to stderr that we are training
                 log(f"‚ö† Model for {symbol} not found. Triggering on-demand training...")
                 log(f"‚è± This will take approximately 15-25 seconds...")
                 
                 # We need to capture the output of training so it doesn't leak to stdout
                 # We can use the same redirect_stdout(sys.stderr) trick
                 
                 success = False
                 with redirect_stdout(sys.stderr):
                     success = train_and_save_model(symbol)
                     
                 if not success:
                     return {"error": f"Failed to train model for {symbol}.", "training": True}
                 
                 # If success, proceed to load logic below
                 log(f"‚úì Training completed for {symbol}. Loading model...")
                 
             except Exception as train_error:
                 return {"error": f"Model not found and training failed: {train_error}", "training": True}

        model = joblib.load(model_path)
        features_list = joblib.load(features_path)
        
        # Fetch data
        end_date = datetime.now().strftime('%Y-%m-%d')
        start_date = (datetime.now() - timedelta(days=90)).strftime('%Y-%m-%d')
        quote = vnstock.Quote(symbol=symbol, source='VCI')
        df_raw = quote.history(start=start_date, end=end_date, interval='1D')
        
        if df_raw.empty:
            return {"error": "No data available"}
            
        # Process features
        df_processed = prepare_features(df_raw, symbol, start_date, end_date, vnstock)
        latest_row = df_processed.iloc[-1]
        
        # Predict
        X = df_processed[features_list]
        latest_features = X.iloc[-1:].values
        prediction = model.predict(latest_features)[0]
        latest_close = latest_row['close']
        
        # Get historical data for chart (last 30 points)
        history_df = df_processed.tail(30).reset_index()
        history_data = []
        for _, row in history_df.iterrows():
            history_data.append({
                "date": str(row['time']).split(' ')[0], # Format: YYYY-MM-DD
                "price": float(row['close'])
            })
        
        # Format result
        result = {
            "symbol": symbol,
            "latest_date": str(latest_row['time']),
            "latest_close": float(latest_close),
            "prediction": float(prediction),
            "change": float(prediction - latest_close),
            "change_pct": float(((prediction - latest_close) / latest_close) * 100),
            "indicators": {
                "rsi": float(latest_row.get('RSI', 0)),
                "macd": float(latest_row.get('MACD', 0)),
                "bb_pos": float(latest_row.get('BB_position', 0))
            },
            "history": history_data
        }
        
        # Top importance
        if hasattr(model, 'feature_importances_'):
            importance = sorted(
                zip(features_list, model.feature_importances_),
                key=lambda x: x[1],
                reverse=True
            )
            result["top_features"] = [
                {"feature": f, "importance": float(i)} for f, i in importance[:5]
            ]
        
        # Save to cache
        result['cached'] = False
        import json
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            log(f"‚úì Cached prediction for {symbol}")
        except Exception as cache_err:
            log(f"‚ö† Failed to save cache: {cache_err}")
            
        return result
        
    except Exception as e:
        return {"error": str(e)}

def get_market_overview():
    """
    Get market overview data (Indices + Top Stocks)
    """
    # Use StringIO to capture any accidental stdout and redirect it to stderr
    with redirect_stdout(sys.stderr):
        try:
            # 1. Get Indices
            indices = ['VNINDEX', 'HNXINDEX', 'UPINDEX']
            indices_data = []
            end_date = datetime.now().strftime('%Y-%m-%d')
            start_date = (datetime.now() - timedelta(days=5)).strftime('%Y-%m-%d')
            
            log(f"üìä Fetching market overview indices...")
            for idx in indices:
                try:
                    quote = vnstock.Quote(symbol=idx, source='VCI')
                    hist = quote.history(start=start_date, end=end_date, interval='1D')
                    if not hist.empty:
                        latest = hist.iloc[-1]
                        prev = hist.iloc[-2] if len(hist) > 1 else latest
                        
                        indices_data.append({
                            "symbol": idx,
                            "price": float(latest['close']),
                            "change": float(latest['close'] - prev['close']),
                            "change_pct": float(((latest['close'] - prev['close']) / prev['close']) * 100)
                        })
                except Exception as e:
                    log(f"‚ö† Warning: Failed to fetch index {idx}: {e}")
                    pass

            # 2. Get Top Stocks (VN30 representative)
            top_symbols = ['VCB', 'VHM', 'VIC', 'HPG', 'FPT', 'MSN', 'MWG', 'VPB', 'TCB', 'VNM']
            stocks_data = []
            
            log(f"üè¢ Fetching top stocks data...")
            for sym in top_symbols:
                try:
                    quote = vnstock.Quote(symbol=sym, source='VCI')
                    hist = quote.history(start=start_date, end=end_date, interval='1D')
                    if not hist.empty:
                        latest = hist.iloc[-1]
                        prev = hist.iloc[-2] if len(hist) > 1 else latest
                        
                        stocks_data.append({
                            "symbol": sym,
                            "price": float(latest['close']),
                            "change": float(latest['close'] - prev['close']),
                            "change_pct": float(((latest['close'] - prev['close']) / prev['close']) * 100),
                            "volume": int(latest.get('volume', 0))
                        })
                except Exception as e:
                    log(f"‚ö† Warning: Failed to fetch stock {sym}: {e}")
                    pass
                    
            return {
                "indices": indices_data,
                "top_stocks": stocks_data,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            log(f"‚ùå Critical error in get_market_overview: {e}")
            return {"error": str(e)}

if __name__ == "__main__":
    import sys
    
    # L·∫•y symbol t·ª´ command line ho·∫∑c d√πng VCB m·∫∑c ƒë·ªãnh
    symbol = sys.argv[1].upper() if len(sys.argv) > 1 else 'VCB'
    
    predict_stock(symbol)
